## üß† LLM Engine - Quick Start

### O que foi implementado

**Novo motor de IA com suporte a Phi-2 via:**
- Transformers (local, recomendado)
- Ollama (opcional, mais r√°pido se j√° tiver)

### Instala√ß√£o

```bash
# J√° inclu√≠do no requirements.txt
pip install -r requirements.txt
```

Se tiver Ollama:
```bash
ollama pull phi
ollama serve  # em outra terminal
```

### Uso via API

```bash
# Status do LLM
curl http://localhost:5000/api/llm/status

# Gerar texto
curl -X POST http://localhost:5000/api/llm/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt":"Ol√°, como", "max_tokens":50}'

# Analisar contexto
curl -X POST http://localhost:5000/api/llm/analyze-context \
  -H "Content-Type: application/json" \
  -d '{
    "text":"Que suspeito",
    "context_keywords":["fake","mente","n√£o acredito"]
  }'

# Limpar cache
curl -X POST http://localhost:5000/api/llm/cache/clear
```

### Uso em Python

```python
from ai.llm_engine import LLMEngine

engine = LLMEngine()

# Gerar
response = engine.generate("Ol√°, como", max_tokens=50)

# Analisar contexto
result = engine.analyze_context(
    "Que suspeito",
    ["fake", "mente", "n√£o acredito"]
)
print(result)  # {"relevant": True/False, "confidence": 0.0-1.0}

# Status
print(engine.get_status())
```

### Backend Strategy

Automaticamente usa (em ordem):
1. **Ollama** se estiver rodando
2. **Transformers** (Phi-2) se dispon√≠vel
3. **Fallback** para sentence-transformers (degradado)

### Arquivos criados/modificados

- ‚úÖ `ai/llm_engine.py` - Motor LLM
- ‚úÖ `tests/test_llm_engine.py` - Testes
- ‚úÖ `core/analyzer.py` - Integrado LLMEngine
- ‚úÖ `web/api_routes.py` - Endpoints REST
- ‚úÖ `requirements.txt` - Deps atualizadas
